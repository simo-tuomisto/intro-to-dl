{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify MNIST digits using Tensorflow. This notebook is based on [mnist_2.0_five_layers_sigmoid.py](https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.0_five_layers_sigmoid.py) by Martin Gorner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(0)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '':\n",
    "    device_name = \"None\"\n",
    "print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll download and extract the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "mnist = mnist_data.read_data_sets(\"data\",\n",
    "                                  one_hot=True,\n",
    "                                  reshape=False,\n",
    "                                  validation_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "We'll start by defining the MLP network as a TensorFlow computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input images and correct labels:\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Variables for weights of two hidden layers:\n",
    "L1, L2 = 200, 100\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L1], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L1]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([L2]))\n",
    "W3 = tf.Variable(tf.truncated_normal([L2, 10], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# The MLP model:\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Ylogits = tf.matmul(Y2, W3) + B3\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# Cross-entropy loss function:\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits,\n",
    "                                                           labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# Prediction accuracy:\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Learning rate and the used optimizer:\n",
    "learning_rate = 0.003\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation graph is launched in a TensorFlow runtime (`tf.Session()`)\n",
    "The weights are initialized with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "We train the network by running the graph with minibatches of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "acc_v,  ce_v = [], []\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100) # minibatch\n",
    "    acc, ce, _ = sess.run([accuracy, cross_entropy, train_step], \n",
    "                          {X: batch_X, Y_: batch_Y})\n",
    "    acc_v.append(acc); ce_v.append(ce)\n",
    "    print(i, \"accuracy:\", acc, ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(iterations), ce_v)\n",
    "plt.title('cross-entropy loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(iterations), acc_v)\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
